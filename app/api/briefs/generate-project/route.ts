/**
 * API de g√©n√©ration de projet √† partir d'un brief
 * 
 * Flow en 3 phases :
 * 1. Analyse du brief avec LLM (GPT-5.1 ou GPT-4o en mode test) ‚Üí JSON structur√©
 * 2. Cr√©ation du projet + n≈ìuds canvas
 * 3. (Optionnel) G√©n√©ration des m√©dias
 * 
 * Streaming SSE pour afficher le raisonnement en temps r√©el
 */

import { NextRequest } from 'next/server';
import OpenAI from 'openai';
import type { GeneratedProjectStructure } from '@/types/generated-project';
import { generateCanvasFromProject } from '@/lib/brief-canvas-generator';

// ========== SYSTEM PROMPT ==========
// Import des configurations par d√©faut
import { 
  DEFAULT_CHARACTER_SYSTEM_PROMPT, 
  DEFAULT_DECOR_SYSTEM_PROMPT,
  DEFAULT_CHARACTER_VARIANT_PROMPTS,
  DEFAULT_DECOR_VARIANT_PROMPTS 
} from '@/lib/brief-defaults';

const SYSTEM_PROMPT_ANALYSIS = `Tu es un sc√©nariste et r√©alisateur expert, dot√© d'une sensibilit√© litt√©raire et cin√©matographique aigu√´.
Tu analyses des briefs cr√©atifs et g√©n√®res une structure de projet compl√®te pour la production vid√©o.

## ARCHITECTURE DU PROJET

### 1. PERSONNAGES - Descriptions exhaustives (SEUL ENDROIT)
Chaque personnage a UN prompt "primary" extr√™mement d√©taill√© d√©crivant son apparence physique compl√®te.
C'est LE SEUL ENDROIT o√π les descriptions physiques apparaissent.

### 2. D√âCORS - Descriptions exhaustives (SEUL ENDROIT)  
Chaque d√©cor a UN prompt "primary" extr√™mement d√©taill√© d√©crivant l'environnement complet.
C'est LE SEUL ENDROIT o√π les descriptions de d√©cor apparaissent.

### 3. PLANS - Trois prompts distincts par plan

#### A. prompt (ACTION VID√âO)
Ce prompt d√©crit l'ACTION, le MOUVEMENT, la PSYCHOLOGIE du plan.
Il sera utilis√© pour animer la vid√©o entre l'image de d√©part et l'image de fin.

**STYLE REQUIS :** Litt√©raire, raffin√©, cin√©matographique.
- Verbes d'action pr√©cis et √©vocateurs
- Mouvements de cam√©ra (travelling, panoramique, plan fixe...)
- Rythme (lent, saccad√©, fluide...)
- Psychologie des personnages (tension, soulagement, h√©sitation...)
- Atmosph√®re (oppressante, l√©g√®re, suspendue...)

**INTERDICTION ABSOLUE :** Ne JAMAIS d√©crire l'apparence physique des personnages ou des d√©cors.
Utiliser uniquement des D√âSIGNATIONS SIMPLES : "l'homme", "la femme", "le vieux", "l'enfant".

**EXEMPLE :**
"L'homme s'avance vers elle d'un pas h√©sitant, le regard fuyant. Elle se retourne lentement. Travelling avant accompagnant le rapprochement, tension croissante dans l'espace qui se r√©duit entre eux."

#### B. promptImageDepart (COMPOSITION VISUELLE D√âBUT)
D√©crit la COMPOSITION SPATIALE de l'image au D√âBUT du plan.
Cette image sera g√©n√©r√©e en 21:9 (cin√©mascope) par √©dition depuis les collections.

**CONTENU :** Position des personnages dans le cadre, rapport au d√©cor, postures.
**STYLE :** Descriptif, spatial, cin√©matographique (comme une indication de mise en sc√®ne).

**EXEMPLE :**
"L'homme de dos au premier plan gauche, face √† la porte. La femme au fond, assise √† son bureau, de profil."

#### C. promptImageFin (COMPOSITION VISUELLE FIN)
D√©crit la COMPOSITION SPATIALE de l'image √† la FIN du plan.
Cette image sera g√©n√©r√©e en 21:9 (cin√©mascope) par √©dition depuis les collections.

**LOGIQUE :** D√âDUIRE cette composition de l'action d√©crite dans le prompt principal.
Si l'action est "l'homme s'approche", la fin montre le r√©sultat de ce rapprochement.

**EXEMPLE :**
"L'homme et la femme face √† face, proches, au centre du cadre. Tension dans leurs regards."

---

## R√àGLES POUR LES PROMPTS PRIMAIRES DE PERSONNAGES
${DEFAULT_CHARACTER_SYSTEM_PROMPT}

## R√àGLES POUR LES PROMPTS PRIMAIRES DE D√âCORS  
${DEFAULT_DECOR_SYSTEM_PROMPT}

---

## FORMAT JSON OBLIGATOIRE

{
  "title": "Titre du projet",
  "synopsis": "Synopsis g√©n√©ral (2-3 phrases)",
  "characters": [
    {
      "id": "perso-prenom",
      "name": "Pr√©nom",
      "description": "Description narrative du personnage",
      "referenceCode": "[PERSO:Pr√©nom]",
      "prompts": {
        "primary": "[DESCRIPTION PHYSIQUE EXHAUSTIVE - 200+ mots minimum]",
        "face": "${DEFAULT_CHARACTER_VARIANT_PROMPTS.face}",
        "profile": "${DEFAULT_CHARACTER_VARIANT_PROMPTS.profile}",
        "back": "${DEFAULT_CHARACTER_VARIANT_PROMPTS.back}"
      }
    }
  ],
  "decors": [
    {
      "id": "decor-nom",
      "name": "Nom du d√©cor",
      "description": "Description narrative du d√©cor",
      "referenceCode": "[DECOR:Nom]",
      "prompts": {
        "primary": "[DESCRIPTION EXHAUSTIVE DU D√âCOR - 150+ mots minimum]",
        "angle2": "${DEFAULT_DECOR_VARIANT_PROMPTS.angle2}",
        "plongee": "${DEFAULT_DECOR_VARIANT_PROMPTS.plongee}",
        "contrePlongee": "${DEFAULT_DECOR_VARIANT_PROMPTS.contrePlongee}"
      }
    }
  ],
  "scenes": [
    {
      "id": "scene-1",
      "sceneNumber": 1,
      "title": "Titre √©vocateur",
      "description": "Synopsis de la sc√®ne",
      "plans": [
        {
          "id": "plan-1-1",
          "planNumber": 1,
          "prompt": "[ACTION LITT√âRAIRE - mouvement, psychologie, cam√©ra - SANS description physique]",
          "promptImageDepart": "[COMPOSITION SPATIALE D√âBUT - positions, postures, rapport au cadre]",
          "promptImageFin": "[COMPOSITION SPATIALE FIN - d√©duite de l'action]",
          "characterRefs": ["perso-prenom"],
          "decorRef": "decor-nom",
          "duration": 5,
          "cameraMovement": "Type de mouvement cam√©ra"
        }
      ]
    }
  ],
  "totalPlans": 4,
  "estimatedDuration": 60
}

## R√àGLES ABSOLUES

1. **S√âPARATION STRICTE** : Descriptions physiques UNIQUEMENT dans les prompts "primary". JAMAIS dans les prompts de plans.

2. **D√âSIGNATIONS SIMPLES** dans les plans : "l'homme", "la femme", "le vieux", "l'enfant" - PAS de descriptions.

3. **COH√âRENCE** : promptImageFin doit √™tre la cons√©quence logique de l'action d√©crite dans prompt.

4. **characterRefs** : Liste des IDs de personnages pr√©sents (peut √™tre vide si plan de d√©cor seul).

5. **decorRef** : ID du d√©cor (obligatoire sauf exceptions).

6. **Prompts variantes** (face, profile, back, angle2, plongee, contrePlongee) : FIXES, ne pas modifier.`;

const SYSTEM_PROMPT_TEST_MODE = `

## ‚ö†Ô∏è MODE TEST ACTIV√â ‚ö†Ô∏è
CONTRAINTES STRICTES √Ä RESPECTER :
- Maximum 2 personnages
- Maximum 1 sc√®ne avec 2 plans maximum
- Chaque prompt doit faire MAXIMUM 3 PHRASES COURTES
- Descriptions simples et directes
- Ne pas d√©tailler excessivement`;

// ========== HELPERS ==========
function sseEvent(type: string, data: Record<string, unknown>): string {
  // On met type en dernier pour qu'il ne soit pas √©cras√© par les donn√©es
  return `data: ${JSON.stringify({ ...data, type })}\n\n`;
}

async function getBriefContent(briefId: string): Promise<{ title: string; content: string } | null> {
  try {
    const baseUrl = process.env.NEXT_PUBLIC_BASE_URL || 'http://localhost:3000';
    const response = await fetch(`${baseUrl}/api/briefs/${briefId}`);
    if (!response.ok) return null;
    
    const data = await response.json();
    
    // Le brief est directement dans data (pas data.brief)
    const brief = data;
    
    // Construire le contenu du brief
    let content = '';
    
    // Ajouter la description du brief
    if (brief.description) {
      content += brief.description + '\n\n';
    }
    
    // Ajouter le contenu des documents texte
    if (brief.documents && Array.isArray(brief.documents)) {
      for (const doc of brief.documents) {
        if (doc.content) {
          content += `--- ${doc.name || 'Document'} ---\n${doc.content}\n\n`;
        }
      }
    }
    
    const finalContent = content.trim();
    
    return {
      title: brief.name || 'Projet sans titre',
      content: finalContent || 'Brief vide - g√©n√®re un projet de d√©monstration simple avec 1 personnage et 1 plan de 5 secondes.',
    };
  } catch (error) {
    console.error('Erreur r√©cup√©ration brief:', error);
    return null;
  }
}

// ========== ROUTE HANDLER ==========
export async function POST(request: NextRequest) {
  const encoder = new TextEncoder();

  try {
    const body = await request.json();
    const { 
      briefId, 
      projectName: customProjectName,
      config,
      isTestMode = false,
    } = body;

    // V√©rifier l'API key OpenAI
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
      return new Response(JSON.stringify({ error: 'OPENAI_API_KEY non configur√©e' }), {
        status: 500,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    // R√©cup√©rer le contenu du brief
    const briefData = await getBriefContent(briefId);
    if (!briefData) {
      return new Response(JSON.stringify({ error: 'Brief non trouv√©' }), {
        status: 404,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    const projectName = customProjectName || `${briefData.title} v1`;

    // Cr√©er le stream SSE
    const stream = new ReadableStream({
      async start(controller) {
        try {
          // D√©terminer le mod√®le √† utiliser
          // En mode test, forcer GPT-4o pour aller vite
          const modelToUse = isTestMode ? 'gpt-4o' : (config?.aiModel || 'gpt-5.1-2025-11-13');
          
          // ========== PHASE 1 : ANALYSE ==========
          controller.enqueue(encoder.encode(sseEvent('phase_start', { 
            phase: 'analysis',
            message: `üß† Phase 1 : Analyse du brief avec ${modelToUse}...`,
          })));

          const openai = new OpenAI({ apiKey });
          
          // Construire le system prompt
          let systemPrompt = config?.systemPrompt || SYSTEM_PROMPT_ANALYSIS;
          if (isTestMode) {
            systemPrompt += SYSTEM_PROMPT_TEST_MODE;
          }

          // Appel LLM avec streaming et reasoning HIGH
          console.log(`[API] Mode test: ${isTestMode}, Mod√®le: ${modelToUse}`);
          const useReasoningAPI = modelToUse.startsWith('gpt-5') || modelToUse.includes('o1') || modelToUse.includes('o3');
          
          let completion;
          if (useReasoningAPI) {
            // GPT-5.1 utilise reasoning_effort
            const reasoningEffort = config?.reasoningLevel || 'high';
            console.log(`[API] Utilisation de ${modelToUse} avec reasoning_effort=${reasoningEffort}`);
            
            completion = await openai.chat.completions.create({
              model: modelToUse,
              reasoning_effort: reasoningEffort as 'low' | 'medium' | 'high',
              max_completion_tokens: 16000, // Augment√© pour les projets avec beaucoup de plans
              messages: [
                { role: 'system', content: systemPrompt },
                { role: 'user', content: `Analyse ce brief et g√©n√®re la structure du projet. IMPORTANT: Cr√©e des prompts PRIMAIRES extr√™mement d√©taill√©s pour chaque personnage et d√©cor.\n\n${briefData.content}` },
              ],
              stream: true,
            } as any); // Type √©tendu pour supporter reasoning_effort
          } else {
            // Mod√®les classiques (GPT-4o, etc.)
            completion = await openai.chat.completions.create({
              model: modelToUse,
              messages: [
                { role: 'system', content: systemPrompt },
                { role: 'user', content: `Analyse ce brief et g√©n√®re la structure du projet :\n\n${briefData.content}` },
              ],
              temperature: 0.7,
              max_tokens: 16000, // Augment√© pour les projets avec beaucoup de plans
              stream: true,
            });
          }

          let fullResponse = '';
          let reasoningContent = '';
          let chunkCount = 0;

          // Stream la r√©ponse de GPT-5.1
          for await (const chunk of completion) {
            chunkCount++;
            const delta = chunk.choices[0]?.delta as any;
            const choice = chunk.choices[0] as any;
            
            // DEBUG: Log les premiers chunks pour voir la structure
            if (chunkCount <= 3) {
              console.log(`[GPT-5.1 DEBUG] Chunk ${chunkCount}:`, JSON.stringify(chunk, null, 2));
            }
            
            // Capturer le reasoning - plusieurs champs possibles selon le mod√®le
            const reasoningText = delta?.reasoning_content || delta?.reasoning || choice?.reasoning_content || choice?.reasoning;
            if (reasoningText) {
              reasoningContent += reasoningText;
              console.log(`[GPT-5.1] Reasoning chunk: ${reasoningText.substring(0, 100)}...`);
            }
            
            // Capturer la r√©ponse finale (le JSON)
            if (delta?.content) {
              fullResponse += delta.content;
            }
          }
          
          console.log(`[GPT-5.1] Total chunks: ${chunkCount}, Reasoning length: ${reasoningContent.length}, Response length: ${fullResponse.length}`);
          
          // Log du reasoning pour debug
          if (reasoningContent) {
            console.log(`[API] Reasoning captur√©: ${reasoningContent.length} caract√®res`);
          }

          controller.enqueue(encoder.encode(sseEvent('phase_complete', { 
            phase: 'analysis',
            message: '‚úÖ Analyse termin√©e',
          })));

          // Parser le JSON
          let projectStructure: GeneratedProjectStructure;
          try {
            // Extraire le JSON de la r√©ponse (au cas o√π il y aurait du texte autour)
            const jsonMatch = fullResponse.match(/\{[\s\S]*\}/);
            if (!jsonMatch) {
              throw new Error('Aucun JSON trouv√© dans la r√©ponse');
            }
            projectStructure = JSON.parse(jsonMatch[0]);
            
            // Sauvegarder le reasoning de GPT-5.1 dans la structure
            if (reasoningContent && reasoningContent.length > 0) {
              projectStructure.reasoning = reasoningContent;
              console.log(`[API] Reasoning sauvegard√©: ${reasoningContent.substring(0, 200)}...`);
            }
          } catch (parseError) {
            console.error('Erreur parsing JSON:', parseError);
            controller.enqueue(encoder.encode(sseEvent('error', { 
              error: 'Erreur de parsing du JSON g√©n√©r√© par l\'IA',
              details: fullResponse.substring(0, 500),
            })));
            controller.close();
            return;
          }

          // ========== PHASE 2 : CR√âATION CANVAS ==========
          controller.enqueue(encoder.encode(sseEvent('phase_start', { 
            phase: 'canvas_creation',
            message: 'üé® Phase 2 : Cr√©ation du canvas...',
          })));

          // R√©cup√©rer les param√®tres vid√©o depuis la config
          const videoCopies = config?.settings?.videoCopies || 4;
          const videoDuration = config?.settings?.videoDuration || 10;
          const videoAspectRatio = config?.settings?.videoAspectRatio || '16:9';

          // G√©n√©rer les n≈ìuds du canvas (avec N n≈ìuds vid√©o par plan et param√®tres)
          const canvasData = generateCanvasFromProject(projectStructure, isTestMode, videoCopies, {
            videoDuration,
            videoAspectRatio,
          });
          
          // Extraire la s√©quence de g√©n√©ration pour plus tard (avec le projet pour les prompts)
          const { getGenerationSequence } = await import('@/lib/brief-canvas-generator');
          const generationSequence = getGenerationSequence(canvasData.structure, projectStructure);

          controller.enqueue(encoder.encode(sseEvent('progress', { 
            progress: 50,
            message: `üì¶ ${canvasData.nodes.length} n≈ìuds cr√©√©s`,
          })));
          
          controller.enqueue(encoder.encode(sseEvent('progress', { 
            progress: 60,
            message: `üîó ${canvasData.edges.length} connexions cr√©√©es`,
          })));

          // IMPORTANT: Le projet est cr√©√© c√¥t√© CLIENT (localStorage)
          // L'API envoie les donn√©es, le client les stocke
          controller.enqueue(encoder.encode(sseEvent('project_data', { 
            projectName,
            canvasData: {
              nodes: canvasData.nodes,
              edges: canvasData.edges,
              viewport: canvasData.viewport,
            },
            projectStructure,
            generationSequence,
          })));

          controller.enqueue(encoder.encode(sseEvent('phase_complete', { 
            phase: 'canvas_creation',
            message: '‚úÖ Canvas cr√©√© avec succ√®s',
            nodeCount: canvasData.nodes.length,
            edgeCount: canvasData.edges.length,
          })));

          // ========== R√âSUM√â FINAL ==========
          // Supporter les deux formats : decors (nouveau) ou locations (ancien)
          const decorsCount = projectStructure.decors?.length || projectStructure.locations?.length || 0;
          
          const summary = {
            projectName,
            characters: projectStructure.characters.length,
            decors: decorsCount,
            locations: decorsCount, // Alias pour r√©trocompatibilit√©
            scenes: projectStructure.scenes.length,
            plans: projectStructure.totalPlans,
            nodes: canvasData.nodes.length,
            edges: canvasData.edges.length,
            // Infos pour g√©n√©ration parall√®le
            // Note: chaque personnage/d√©cor a 1 image primaire + 3 variantes = 4 images
            imagesToGenerate: generationSequence.characterImages.reduce((acc, c) => acc + c.imageNodeIds.length, 0) +
                              (generationSequence.decorImages?.reduce((acc, d) => acc + d.imageNodeIds.length, 0) || 
                               generationSequence.locationImages.reduce((acc, l) => acc + l.imageNodeIds.length, 0)),
            videosToGenerate: generationSequence.videos.length,
            quality: config?.quality || 'elevee',
          };

          controller.enqueue(encoder.encode(sseEvent('complete', { 
            message: 'üéâ Projet g√©n√©r√© avec succ√®s !',
            summary,
            generationSequence,
          })));

          controller.close();
        } catch (error: unknown) {
          console.error('Erreur g√©n√©ration:', error);
          controller.enqueue(encoder.encode(sseEvent('error', { 
            error: error instanceof Error ? error.message : 'Erreur inconnue',
          })));
          controller.close();
        }
      },
    });

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache, no-transform',
        'Connection': 'keep-alive',
      },
    });
  } catch (error: unknown) {
    console.error('Erreur API:', error);
    return new Response(JSON.stringify({ 
      error: error instanceof Error ? error.message : 'Erreur inconnue' 
    }), {
      status: 500,
      headers: { 'Content-Type': 'application/json' },
    });
  }
}

